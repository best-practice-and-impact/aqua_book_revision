::: {.callout-important}
This version of the AQuA book is a preliminary ALPHA draft.  It is still in development, and we are still working to ensure that it meets user needs. 

The draft currently has no official status. It is a work in progress and is subject to further revision and reconfiguration (possibly substantial change) before it is finalised. 
:::

# Design

## Introduction and summary

This chapter describes how the Analyst translates the scope for the analysis agreed with the Commissioner into the Design, an actionable plan of work. It sets out recommended practices for documenting the Design as an Analytical Plan, making changes to the plan and keeping an audit trail of those changes, testing for reproducibility and uncertainty, and agreeing this with the Approver. An Analysis Steering Group can be an effective means for communications about the Design.

### The Commissioner's responsibilities during design
The Commissioner reviews the Plan and agrees that what the Analyst is going to do will satisfy the Commissioner’s need.

### The Analyst's responsibilities during design
The Analyst should develop the method and plan how to address the Commissioner’s needs, what the assurance requirements will be and plan for sufficient time for the assurance activity.    

### The Assurer's responsibilities during design
The Assurer should agree with Analyst and Commissioner what the assurance requirements will be and plan sufficient time for the assurance activity.    

### The Approver's responsibilities during design
The Approver should document the new project and ensure that required mechanisms are in place for the appropriate assurance to take place. For example, they should ensure that the Analyst and Assurer are aware of local assurance protocols.    

## Designing the analysis

The Analyst translates the scope for the analysis agreed with the Commissioner into an actionable plan of work.   The Engagement and Scoping stage provides the Analyst with an understanding of the Commissioner's requirements in terms of outputs to be delivered, including acceptable levels of accuracy, precision and margins of error. During the Design phase, the Analyst will translate the delivery of the agreed outputs into an Analytical Plan. The Analytical Plan should consider:

* Methodology for producing results, including the treatment of uncertainty;
* Project management approach (for example Agile, Waterfall or a combination of approaches);
* Sourcing of inputs and assumptions;
* Data and file management;
* Change management and version control;
* Code management, documentation and testing;
* Communication between stakeholders;
* Quality assurance procedures during the project lifetime;
* Documentation to be delivered;
* Process for updating the Analytical Plan;
* Ethics;
* Reporting;
* Downstream application.

Documentation of the Analytical Plan should be proportionate to the Design. For larger or more complex projects, the Analyst might request assurance of the Analytical Plan.

## Testing the methodology

Best practice is to dry run with independent assurance of the proposed approach to see if the plan:    

* Delivers as intended 
* Adequately addresses the complexities of the customer issue for this purpose 
* Is well-structured for the purpose, data driven, and reflects a robust overall design.

It is good practice to engage subject matter experts in this review, independent assurance, and to ensure the accuracy and limitations of the chosen methods are understood, ideally with tests baselining their response against independent reference cases.

If the analysis is written in code, this should include appropriate testing to ensure that the code works as intended to deliver expected outcomes. Following [recommended good practices for code development](https://best-practice-and-impact.github.io/qa-of-code-guidance/intro.html) is the best way to ensure that code is fit for purpose.

On completion of the Design phase, the Assurer should be aware of the quality assurance tasks that will be required of them during the project lifetime and have assured the necessary elements of the Analytical plan.

Iteration between the Commissioner and the Analyst is normal and expected whilst the analytical design develops. Considering analytical quality assurance and the acceptable margin of error for the Commissioner is essential to define how good is good enough. 

Agreeing the extent of the importance of the analysis output to decision making will set the level of [materiality](https://en.wikipedia.org/wiki/Materiality_(auditing)) (see also Definitions and Key Concepts), which will indicate what would be proportionate assurance.

The Commissioner remains important through the analysis cycle, since they may well have greater expertise in the subject than the Analyst. Their contribution towards the input assumptions, data requirements and the most effective ways to present the outputs can prove invaluable.

The Analytical plan should be updated with formal version control to reflect revisions to the design including assurance activities.  

## Documenting the design of the analysis

Best practice is to document the design process recording how the proposed analytical process generates the requested insights. This plan should be supported by the design documentation.

## Analysing uncertainty

During the design phase, Analysts should examine the planned analysis systematically for possible sources and types of uncertainty, to maximise the chance of identifying all that are sufficiently large to breach the acceptable margin of error.

Analysts should make sure that they report identified sources of uncertainty appropriately and proportionately to enable an assessment of whether results are within the Commissioner’s acceptable margin of error.

The [Uncertainty Toolkit for Analysts](https://analystsuncertaintytoolkit.github.io/UncertaintyWeb/index.html) provides advice on the treatment of uncertainty. Other authorities provide guidance for specific purposes, such as the [Office for Statistics Regulation's Approaches to presenting uncertainty in the statistical system](https://osr.statisticsauthority.gov.uk/publication/approaches-to-presenting-uncertainty-in-the-statistical-system/pages/1/).

## Reproducible analytical pipelines

The recommended approach for developing analysis in code is to use a [Reproducible Analytical Pipeline (RAP)](https://analysisfunction.civilservice.gov.uk/support/reproducible-analytical-pipelines/). Reproducible Analytical Pipelines shall:

* Follow the practices set out in the [Analysis Function Quality Assurance of Code for Analysis and Research manual](https://best-practice-and-impact.github.io/qa-of-code-guidance/intro.html).
* Meet the requirements of the [Reproducible Analytical Pipelines minimum viable product](https://github.com/best-practice-and-impact/rap_mvp_maturity_guidance/blob/master/Reproducible-Analytical-Pipelines-MVP.md).

## Artificial intelligence and machine learning

Artificial Intelligence (AI) and some classes of Machine Learning (ML)  methods have opaque algorithms processing their inputs. These belong to a class of models known as “black boxes” - devices, systems, or objects which produce useful information without revealing any information about their internal workings.

Using a black box model places greater weight on the design of the analysis and the assurance and validation of outputs by domain experts, particularly Assurers, Approvers and Commissioners.    [Validate AI](https://validateai.org/)  offers resources.
